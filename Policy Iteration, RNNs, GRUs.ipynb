{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ad7616",
   "metadata": {
    "id": "ab0544a4"
   },
   "source": [
    "## Problem 1: Reinforcement Learning: Policy Iteration (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2c4f1",
   "metadata": {
    "id": "cbd0edfd"
   },
   "source": [
    "1. Suppose we have a policy $\\pi$ and a monotonically improved policy $\\pi'$, such that for all states $s \\in S$, we have \n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s, \\pi'(s)) \\ge V_{\\pi}(s).\n",
    "$$\n",
    "\n",
    "Show that the value function $V_{\\pi'}$ dominates $V_{\\pi}$, i.e., for all states $s \\in S$\n",
    "\n",
    "$$\n",
    "V_{\\pi'}(s) \\ge V_{\\pi}(s).\n",
    "$$\n",
    "\n",
    "*hint: Consider expanding the value functions iteratively.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff5b95",
   "metadata": {
    "id": "5a7b2bda"
   },
   "source": [
    "2. Apply the conclusion of part 1 to the policy improvement step to show that it leads to a sequence of monotonically improving policies. In other words, show that if $\\pi \\neq \\pi^*$, the next round policy $\\pi'$ under the policy iteration algorithm satisfies $V_{\\pi'}(s) > V_{\\pi}(s)$ for some state $s\\in S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad3df3",
   "metadata": {
    "id": "eede565f"
   },
   "source": [
    "![](https://rstr.in/avjn9edat3zttr/my-library/DpCIEHmbGUj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88022d1e",
   "metadata": {},
   "source": [
    "worked with valentina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c317b",
   "metadata": {
    "id": "78f64b66"
   },
   "source": [
    "## Problem 2: Elephants Can Remember (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9104ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/85/15/cf99a373812d37f8ae99752a34a9f5f690d820ceb5b302e922705bc18944/tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-macos==2.15.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-macos==2.15.0 from https://files.pythonhosted.org/packages/eb/9f/0759e2fea4a3c48f070b64811c2c57036b46353ba87263afc810b8f4188a/tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.9.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/32/1f/981809b77b71972beec34b3ff5422c1b1f7e519daac7b3cbd055c05ba2cf/libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes~=0.2.0 from https://files.pythonhosted.org/packages/15/da/43bee505963da0c730ee50e951c604bfdb90d4cccc9c0044c946b10e68a7/ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.25.1)\n",
      "Requirement already satisfied: setuptools in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/5b/e9/1444afc87596a90066704cc46ed661a4e7b348eec03a3fc2ca10ab917254/tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/92/93/3cbc00a269b46277ff26355074a8315eeb4c87240c27d6f7efeabe818fd9/grpcio-1.59.3-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
      "  Downloading grpcio-1.59.3-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.16,>=2.15 from https://files.pythonhosted.org/packages/6e/0c/1059a6682cf2cc1fcc0d5327837b5672fe4f5574255fa5430d0a8ceb75e9/tensorboard-2.15.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.16,>=2.15.0 from https://files.pythonhosted.org/packages/b6/c8/2f823c8958d5342eafc6dd3e922f0cc4fcf8c2e0460284cc462dae3b60a0/tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.16,>=2.15.0 from https://files.pythonhosted.org/packages/fc/a7/0d4490de967a67f68a538cc9cdb259bff971c4b5787f7765dc7c8f118f71/keras-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/aj/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/1f/eb/29123fbd92e4cb25d24713ab5d26ea74e02ce04290edc7c35356441de4f2/google_auth-2.25.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.25.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth-oauthlib<2,>=0.5 from https://files.pythonhosted.org/packages/ce/33/a907b4b67245647746dde8d61e1643ef5d210c88e090d491efd89eff9f95/google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/cb/d3/a164038605494d49acc4f9cda1c0bc200b96382c53edd561387263bb181d/protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/aj/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/aj/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aj/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aj/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aj/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aj/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/aj/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/aj/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Downloading tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl (208.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.8/208.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.59.3-cp311-cp311-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_12_0_arm64.whl (1.9 MB)\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading google_auth-2.25.1-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.2/184.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.1\n",
      "    Uninstalling protobuf-4.25.1:\n",
      "      Successfully uninstalled protobuf-4.25.1\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.25.1 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-macos-2.15.0 termcolor-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeadc187",
   "metadata": {
    "id": "0804721c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import random\n",
    "  \n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19a842b",
   "metadata": {
    "id": "f93de082"
   },
   "outputs": [],
   "source": [
    "with open('q2_data/Agatha_Christie_train.txt', 'r') as file:\n",
    "    train_text = file.read()\n",
    "    \n",
    "with open('q2_data/Agatha_Christie_test.txt', 'r') as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "vocabulary = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Dictionaries to go from a character to index and vice versa\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55beab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11640213",
   "metadata": {
    "id": "7320ed64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrs. oliver looked at herself in the glass. she gave a brief, sideways look towards the clock on the mantelpiece, which she had some idea was twenty minutes slow. then she resumed her study of her coiffure. the trouble with mrs. oliver was--and she admitted it freely--that her styles of hairdressing were always being changed. she had tried almost everything in turn. a severe pompadour at one time, then a wind-swept style where you brushed back your locks to display an intellectual brow, at least'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first 500 characters of our training set\n",
    "train_text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2a5875",
   "metadata": {
    "id": "3a50abac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 44 characters\n",
      "The training set contains 262174 characters\n",
      "The test set contains 7209 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains\", vocab_size, \"characters\")\n",
    "print(\"The training set contains\", len(train_text) ,\"characters\")\n",
    "print(\"The test set contains\", len(test_text) ,\"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a6b67",
   "metadata": {
    "id": "d02ce399"
   },
   "source": [
    "### Problem 2.1: The Diversity of Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae5263",
   "metadata": {
    "id": "06832930"
   },
   "source": [
    "#### Vanilla RNNs\n",
    "\n",
    "Simplicity: Vanilla RNNs have a relatively simple structure. They consist of a single hidden layer with a recurrent connection that feeds the hidden state from one step of the sequence to the next.\n",
    "\n",
    "Issue with Long-Term Dependencies: Vanilla RNNs struggle with capturing long-term dependencies in a sequence. This is primarily due to the vanishing gradient problem, where gradients become exponentially smaller as they are propagated back through each time step. As a result, it becomes difficult for the RNN to learn and maintain information from earlier time steps in longer sequences.\n",
    "\n",
    "Training Difficulty: Due to the vanishing gradient problem, training vanilla RNNs on long sequences is challenging, as the network becomes less sensitive to input and changes in parameters for early elements in the sequence.\n",
    "\n",
    "#### GRUs\n",
    "\n",
    "Complex Architecture with Gates: GRUs are an advancement over vanilla RNNs. They incorporate gating mechanisms - specifically, the update gate and reset gate - which control the flow of information.\n",
    "Update Gate: The update gate in a GRU helps the model determine how much of the past information (from previous time steps) needs to be passed along to the future. This is crucial for the model to 'remember' relevant information over long sequences.\n",
    "\n",
    "Reset Gate: The reset gate decides how much past information to forget. This is useful when the sequence has segments with little or no relevance to each other, allowing the model to drop irrelevant information from the past.\n",
    "\n",
    "Addressing Vanishing Gradient Problem: The gating mechanisms in GRUs help mitigate the vanishing gradient problem seen in vanilla RNNs. By adaptively choosing which information to pass through, GRUs can maintain relevant information over long sequences, making them more effective for tasks involving long-term dependencies.\n",
    "\n",
    "Efficiency in Learning Long Sequences: Due to their gating mechanisms, GRUs are generally more efficient and perform better than vanilla RNNs in learning from long sequences. They are capable of capturing dependencies from a larger range of time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e246465",
   "metadata": {
    "id": "ccac684e"
   },
   "source": [
    "### Problem 2.2: Generating Text with the Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463c1c0",
   "metadata": {
    "id": "8b07db8a"
   },
   "source": [
    "1. Character-based models are not limited by a fixed vocabulary, unlike word-based models which can struggle with words not seen during training. In word-based models, any word not in the training vocabulary is typically treated as an 'unknown' or OOV token, which limits the model's ability to understand or generate those words. Character-based models, on the other hand, can handle any word, even if it was not seen during training, as long as the characters making up the word are known.\n",
    "\n",
    "2. Character-based models can learn and generate new, plausible word forms that may not have been present in the training data. This is because they model the language at a more granular level, capturing underlying patterns in how characters combine to form words. This could be particularly useful in applications like poetry, rhyme, or prose generation, or in languages where compound words are common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2d7f16",
   "metadata": {
    "id": "45bd5d3b"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('q2_data/RNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "RNN_model = model_from_json(loaded_model_json)\n",
    "RNN_model.load_weights(\"q2_data/RNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "589f537c",
   "metadata": {
    "id": "648273ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Vanilla_RNN_1 (SimpleRNN)   (None, 100, 128)          22144     \n",
      "                                                                 \n",
      " Vanilla_RNN_2 (SimpleRNN)   (None, 64)                12352     \n",
      "                                                                 \n",
      " Dense_layer (Dense)         (None, 44)                2860      \n",
      "                                                                 \n",
      " Softmax_layer (Activation)  (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37356 (145.92 KB)\n",
      "Trainable params: 37356 (145.92 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_RNN = RNN_model.get_weights()\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3852f17d",
   "metadata": {
    "id": "736ff633"
   },
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = prompt\n",
    "    \n",
    "    # Function to convert character to one-hot vector\n",
    "    def char_to_onehot(char):\n",
    "        onehot = np.zeros((vocab_size, 1))\n",
    "        onehot[char_to_indices[char]] = 1\n",
    "        return onehot\n",
    "\n",
    "    # Function for softmax\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    for i in range(N - len(prompt)):\n",
    "        # Get the input character\n",
    "        x = char_to_onehot(output_sentence[-1]) if i > 0 else char_to_onehot(prompt[0])\n",
    "\n",
    "        # First Recurrent Layer\n",
    "        h1 = np.tanh(np.dot(W_xh1, x) + np.dot(W_h1h1, h1) + b_h1)\n",
    "\n",
    "        # Second Recurrent Layer\n",
    "        h2 = np.tanh(np.dot(W_h1h2, h1) + np.dot(W_h2h2, h2) + b_h2)\n",
    "\n",
    "        # Output Layer\n",
    "        y = np.dot(W_h2y, h2) + b_y\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Sample a character from the probability distribution\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        output_sentence += indices_to_char[idx]\n",
    "\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a505803a",
   "metadata": {
    "id": "12458ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. she gave a brief, sideways lookand or somother who had beendrements. she mateesthard the ladd the was a maid, i bely as orcoughtous to askitt you going coxtaly. theing of serso.\" \"yes, no no. not shive? she suplofe muringany time invily yellntale are at wantiduse it sascilious and thought there was anly a rowadd foor and they asor for and sormon. alint: said then i such whom a surt anyowy pose anyon,.\" \"yes, she cailst, of cor's whow you said pave for a mrs. oliver baskby mrs. oliver. oh, you?\" \"yes, someone shameectul, is as such other there were he fion in aly will be wis simely wills quicadred a frouk ham mantersulf oher the fame nations were expmened her mothers and triep and explened lithoride. ane, i've you telnought exteclearint it any oldoy in the cheridencably to anywhat anything. nimily, he was madnere noo.\" \"they raglly wyone wan the came you loubor, of celia realy,\" said poirot. camirat of ollcagre.\" \"and you mentalowe mightold an\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_RNN(weights_RNN, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look', \n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e7099",
   "metadata": {
    "id": "c950eae8"
   },
   "source": [
    "### Problem 2.3: Generating Text with the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a418e51",
   "metadata": {
    "id": "82291909"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('q2_data/GRU_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "GRU_model = model_from_json(loaded_model_json)\n",
    "GRU_model.load_weights(\"q2_data/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5831d54",
   "metadata": {
    "id": "738974b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 512)               857088    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 44)                22572     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 879660 (3.36 MB)\n",
      "Trainable params: 879660 (3.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_GRU = GRU_model.get_weights()\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7536086e",
   "metadata": {
    "id": "259c1d40"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "        \n",
    "     # Helper function for softmax\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    # Function to convert character to one-hot vector\n",
    "    def char_to_onehot(char):\n",
    "        onehot = np.zeros((vocab_size, 1))\n",
    "        onehot[char_to_indices[char]] = 1\n",
    "        return onehot\n",
    "\n",
    "    output_sentence = prompt\n",
    "\n",
    "    for i in range(N - len(prompt)):\n",
    "        # Get the input character\n",
    "        x = char_to_onehot(output_sentence[-1]) if i > 0 else char_to_onehot(prompt[0])\n",
    "\n",
    "        # GRU Forward Pass\n",
    "        u = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        r = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        h_tilde = np.tanh(np.dot(W_hx, x) + r * (np.dot(W_hh, h)) + b_h)\n",
    "        h = u * h + (1 - u) * h_tilde\n",
    "\n",
    "        # Output Layer\n",
    "        y = np.dot(W_y, h) + b_y\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Sample a character from the probability distribution\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        output_sentence += indices_to_char[idx]\n",
    "\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "456eb344",
   "metadata": {
    "id": "3fc14584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. she gave a brief, sideways looky to her so readly the dobto that. i to going the naverally deat on the finger to it. in to call the ation at the time or youred the warkerite of sef she was at eeepll showing will hive he will call living the aidepoilot the donot along goron ano souts of celie had interestion putternt and whe that she had a get an son a sthacis and the wefe. i mean the stway was he wanted to lear that see worried. it's a troubligs this suseere aill dis in the went all about. the ho tooull you ask a young moment or truat from oriones ard elephants and i had mide the dount gitl dearing in erea list, some ixa ten uple tig things they were not snowe him. but there ask there is as it listed telbe how inferettence. and ame. at the eamy a pimp rime mo gaing i a to tay it, but all ary old quite a tranked at about it or me the rather nive. i seetlight tells that's the other recently dades, a telline, because it they we loor that. some h\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_GRU(weights_GRU, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553dd6b",
   "metadata": {
    "id": "3ccf8083"
   },
   "source": [
    "### Vanilla RNN Output: \n",
    "\n",
    "This output exhibited less coherence in terms of sentence structure and logical progression of ideas. Vanilla RNNs can struggle with long-term dependencies due to issues like vanishing gradients, making it harder for them to maintain context over longer sequences. There are more repetitive patterns and broken sentence structures, as the model may lose track of what it has already generated. Here we see it struggles more with maintaining consistent use of named entities. Characters and subjects seem to appear and change abruptly without clear context. The text also exhibits a certain level of repetitiveness and abrupt topic shifts (\"she cailst, of cor's whow you said pave for a mrs. oliver baskby mrs. oliver\"). The output displays a somewhat disjointed and erratic flow of ideas. The sentences are more fragmented, and there are noticeable grammatical inconsistencies and nonsensical phrases (\"mateesthard the ladd the was a maid\", \"yellntale are at wantiduse\"). It suggests a struggle to maintain context and a coherent narrative over the sequence. Clearly higher perplexity.\n",
    "\n",
    "\n",
    "### GRU Output: \n",
    "\n",
    "This output generated far less gibberish and seemed to have more contextually relevant words over various iterations. This is likely because GRUs, with their gating mechanisms are generally better at capturing long-term dependencies in sequences. The text shows a somewhat better grasp of character and subject continuity, although it's not perfect. While there is some repetition, it's less pronounced. The GRU model shows a better ability to vary sentence structure and content, which can be attributed to its more sophisticated handling of sequence dependencies.The text appears more coherent and maintains a clearer narrative structure. While there are still some grammatical errors and odd phrases, the sentences are more structured and logical (\"may she things with some interesten to me semition and the sime by scaroied, but whem canell from i who leove now she looked at you?\" \"oh, yes, she looked a oni?. i shill remember to know any owe now fir this. i suppose i well to me?\"). This form of text has a much higher occurance of sensible words and sentence structure. This indicates a better handling of context and language rules. Clearly lower perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9271e3",
   "metadata": {
    "id": "5c9a5bf5"
   },
   "source": [
    "2. Calculate the perplexity of each language model by using test_text, an unseen extract of the book. Choose the prompt as the first $m$ letters of the test set, where $m$ is a parameter that you can choose yourself. You should be able to reuse the majority of your previous code in this calculation. Discuss your results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9eb1821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = prompt\n",
    "    sum_log_probs = 0 \n",
    "    \n",
    "    def char_to_onehot(char):\n",
    "        onehot = np.zeros((vocab_size, 1))\n",
    "        onehot[char_to_indices[char]] = 1\n",
    "        return onehot\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    for i in range(N - len(prompt)):\n",
    "        # Get the input character\n",
    "        x = char_to_onehot(output_sentence[-1]) if i > 0 else char_to_onehot(prompt[0])\n",
    "\n",
    "        # First Recurrent Layer\n",
    "        h1 = np.tanh(np.dot(W_xh1, x) + np.dot(W_h1h1, h1) + b_h1)\n",
    "\n",
    "        # Second Recurrent Layer\n",
    "        h2 = np.tanh(np.dot(W_h1h2, h1) + np.dot(W_h2h2, h2) + b_h2)\n",
    "\n",
    "        # Output Layer\n",
    "        y = np.dot(W_h2y, h2) + b_y\n",
    "        p = softmax(y)\n",
    "\n",
    "        # calculate log probability of actual next character\n",
    "        if i < len(test_text) - len(prompt):\n",
    "            actual_next_char = test_text[len(prompt) + i]\n",
    "            actual_next_char_idx = char_to_indices[actual_next_char]\n",
    "            prob = p[actual_next_char_idx]\n",
    "            sum_log_probs += np.log(prob + 1e-9)  # avoid log(0)\n",
    "        \n",
    "        # Sample a character from the probability distribution\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        output_sentence += indices_to_char[idx]\n",
    "\n",
    "    return output_sentence, sum_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a711c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Your code starts here\n",
    "    output_sentence = \"\"\n",
    "        \n",
    "     # Helper function for softmax\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    # Function to convert character to one-hot vector\n",
    "    def char_to_onehot(char):\n",
    "        onehot = np.zeros((vocab_size, 1))\n",
    "        onehot[char_to_indices[char]] = 1\n",
    "        return onehot\n",
    "\n",
    "    output_sentence = prompt\n",
    "    sum_log_probs = 0\n",
    "\n",
    "    for i in range(N - len(prompt)):\n",
    "        # Get the input character\n",
    "        x = char_to_onehot(output_sentence[-1]) if i > 0 else char_to_onehot(prompt[0])\n",
    "\n",
    "        # GRU Forward Pass\n",
    "        u = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        r = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        h_tilde = np.tanh(np.dot(W_hx, x) + r * (np.dot(W_hh, h)) + b_h)\n",
    "        h = u * h + (1 - u) * h_tilde\n",
    "\n",
    "        # Output Layer\n",
    "        y = np.dot(W_y, h) + b_y\n",
    "        p = softmax(y)\n",
    "        \n",
    "        # calculate log probability of actual next character\n",
    "        if i < len(test_text) - len(prompt):\n",
    "            actual_next_char = test_text[len(prompt) + i]\n",
    "            actual_next_char_idx = char_to_indices.get(actual_next_char, None)\n",
    "            if actual_next_char_idx is not None:\n",
    "                prob = p[actual_next_char_idx]\n",
    "                sum_log_probs += np.log(prob + 1e-9)  # avoid log(0)\n",
    "\n",
    "\n",
    "        # Sample a character from the probability distribution\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        output_sentence += indices_to_char[idx]\n",
    "        \n",
    "\n",
    "    return output_sentence, sum_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ede8f00",
   "metadata": {
    "id": "e0c2a390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Vanilla RNN: 1126.5341368285174\n",
      "Perplexity of GRU: 540.5134506477787\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model_function, weights, prompt, test_sequence):\n",
    "    # Generate text using the model\n",
    "    generated_text, log_probs = model_function(weights, prompt, len(test_sequence))\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = np.exp(-np.sum(log_probs) / len(test_sequence))\n",
    "    return perplexity\n",
    "\n",
    "# Choose a prompt length\n",
    "m = 50  # For example, the first 50 characters\n",
    "prompt = test_text[:m]\n",
    "\n",
    "# Compute perplexity for both models\n",
    "perplexity_RNN = calculate_perplexity(sample_text_RNN, weights_RNN, prompt, test_text)\n",
    "perplexity_GRU = calculate_perplexity(sample_text_GRU, weights_GRU, prompt, test_text)\n",
    "\n",
    "# Print results\n",
    "print(f\"Perplexity of Vanilla RNN: {perplexity_RNN}\")\n",
    "print(f\"Perplexity of GRU: {perplexity_GRU}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a642968",
   "metadata": {},
   "source": [
    "#### As discussed before, GRU expectedly has lower perplexity, indicating a better model. This is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9959fca",
   "metadata": {
    "id": "8df914d2"
   },
   "source": [
    "Clearly, we saw that the GRU approach is much much better. However we can improve upon this...\n",
    "\n",
    "Implementing a bidirectional rather than unidirectional feed-forward GRU as implemented could be beneficial. The most significant advantage of a bidirectional GRU is its ability to process the input sequence from both forward and backward directions. This means the model would capture information from both past and future contexts relative to a given point in the sequence, leading to better entity recognition, sentiment analysis, and part-of-speech tagging. The context available on both sides of a given word or token can be crucial for accurate predictions.\n",
    "\n",
    "In addition, implementing attention mechanisms could help the model better determine the relevant parts of input and develop a more contextually appropriate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a093c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
